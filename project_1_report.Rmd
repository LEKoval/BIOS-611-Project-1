
Lauren Koval

BIOS 611

November 15, 2020


---
title: "Examination of 5 Causes of Death Worldwide from 1980-2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project Overview

This project set out to uncover elementary spatial and temporal relationships between various causes of non-disease deaths and the frequency with which they occurred in countries all over the world from 1980-2017.

The datasets I used are publicly available for download on Kaggle. Links are available through the **README.md** file on github.


[global causes of deaths other than disease](https://www.kaggle.com/tahminashoaib86/global-cause-of-the-deaths-other-than-diseases)-  This dataset contains the country, ISO Code, year, total population, and annual number of deaths due to conflicts and terrorism, epidemics, famine, natural disaster, & other injuries for each country in each year. Additional information that I did not use in my analysis but is present in the file is the male population, female population, GDP, and GDP per capita for each country in each year.

[country to continent mapping](https://www.kaggle.com/statchaitya/country-to-continent)-  This dataset contains the country, various identifying country codes, including the ISO code, here named *code_3*, as well as subregions and continents.


To start, I generated a few preliminary figures with the script **prelim_figs.R**:

![](figures/cause_line_prelim.png)

This line graph shows the global number of deaths from the five major causes examined in this analysis each year from 1980- 2017. 

![](figures/region_death_heat_prelim.png)

 This heatmap shows the total number of deaths from the five major causes examined in this analysis in various subregions of the world from 1980-2017

Unfortunately, neither of these figures proved to be overly informative. The range of the data was so great, it was difficult to scale the figures while just considering the total number of deaths. The main aim of this project was to to recreate these figures in ways that would be more insightful.


## Analysis

### Fixing Population Data

I first wanted to consider the ratio of the number of deaths to the population. For the line graph, this would be the annual global population, and for the heatmap, this would be the subregion population. The subregion population consideration was later dropped. I will elaborate more on this later. Unfortunately, the population data in the cause dataset was incomplete. Exploring the inconsistencies and testing ways to address this issue ended up becoming a more time consuming and significant portion of the project than I initially anticipated. The script **fix_pop_data.R** cleans and organizes the dataset then fixes population data where necessary. 

Upon investigation, 182 of the 194 countries were missing the population for the epidemics cause for every year. This was rectified by copying the population from another cause for that country and that year. There were 8 countries that were missing all population data and these were omitted from the analysis (American Samoa, Bermuda, Dominica, Greenland, Marshall Islands, Northern Mariana Islands, Taiwan, & U.S. Virgin Islands). The other 4 countries (Eritrea, Kuwait, Palestine, and Serbia) were missing population data for a handful of sequential years. Ultimately, I attempted to fit various models to the existing population data for those countries to fill in this data.

![](figures/eritrea_population.png)

![](figures/kuwait_population.png)
![](figures/palestine_population.png)

![](figures/serbia_population.png)

There was a level of subjectivity when selecting the appropriate model to use as a predictor for Eritrea and Serbia. It wouldn't surprise me if there exists a population specific model that could be better predictor than the quadratic and quartic polynomials that I have chosen to use here. However, I have some concerns about the overall quality of the data which I believe to be the ultimate limiting factor, so I am okay with using less precise models. The final population data used in further analyses for each of these countries is comprised of the reported values with the predicted values for the missing years.

**fix_pop_data.R** produces the above figures as well as a csv of the final country specific dataset for each of the modeled countries (**adjusted_eritrea_population.csv**, **adjusted_kuwait_population.csv**, **adjusted_palestine_population.csv**, and **adjusted_serbia_population.csv**) and a final csv of comprising the new complete cause dataset of 186 countrues to use for further analyses (**cleaned_pop_data.csv**). All of these files can be found in the *derived_data* folder. 


### Findings

After creating a more complete and unified cause dataset, I attempted to recreate the preliminary figures but scaling for population size with the script **updated_prelim_figs.R**. This may have been a futile effort. 

![](figures/deaths_population_ratio_line_graph.png)

The line graph generated was no more informative than the initial figure. Additionally, after consideration, it didn't make much sense to scale the subregion heatmap by population since there was no temporal consideration presented in the initial figure. It felt contrived and uninintuitive to artificially introduce that component by aggregating the populations of all subregions over a 38 year span. Instead I considered another approach: scaling by the total number of deaths. Since death is a discrete event, it is easier to aggregate over time than a fluctuating variable like population. I then generated these figures:

![](figures/deaths_ratio_line_graph.png)

![](figures/deaths_ratio_heatmap.png)

These figures indicate that both spatially and temporally, Conflicts and Terrorism generally account for more deaths, relative to the number of deaths, than the other four causes. It is particularly interesting to note the mirroring effect of Conflicts and Terrorism with Natural Disaster in the line graph. Upon inspection, in years where a greater proportion of deaths occurred due to Conflict and Terrorism, a smaller proportion of deaths occurred due to Natural Disaster. I would be very interested in further exploring this relationship. Additionally, just out of curiosity, I would be interested in using a time series analysis to predict the number of deaths expected in the U.S. due to an epidemic for 2020 solely to compare to where we are at (~220K as of October 19th) 

I did have some reservations about the quality of the data in the cause dataset, so it would be beneficial to try to find additional datasets that could be used to both validate and supplement the current data for future analyses.


## The Interactive Visualization

An Rshiny application was later created that was inspired by the line graph. While the line graph that was scaled by the number of deaths was more insightful then the initial figure, it was still a little difficult to look at,as there is a lot going on. I thought a bar graph depicitng the same information for each year would be more straightforward. I implemented a sliding bar to change the year which allows for a smooth, seamless transition between the graphs for each year. I think both the static and interactive plots have their merit, but the interactive plot makes the information easier to digest.

The application can be run by following the instructions in the section **Rshiny Visualization** in the project README. 


## The Python Component

For the Python component of this project, I attempted to perform a time series analysis that forecasted the number of deaths the United States would experience as a result of an epidemic in 2020. As a reminder, the dataset I worked with contained the number of deaths due to conflict and terrorism, famine, epidemics, natural disaster, and other injuries in 194 countries from 1980-2017. Not surprisingly, the forecasted value from this dataset for 2020 was nowhere close to the ~245k we are at today (11-15-20). In fact it isn't even practical.

To perform the analysis I build an additive Autorgressive Integrated Moving Average model using the SARIMAX class within the statsmodels library. I am not overly familiar with time series analysis and this was my first time attempting anythig of the sort, so this is likely not the most refined model. Nonetheless, it was good experience to read the documentation and piece everything together on my own.


Here is the the initial plot showing the number of deaths in the United States from an epidemic from 1980-2017

![](figures/usa_epidemics_trend.png)


Next I decomposed the data into the trend, seasonal trend, and the residuals, which are important factors to consider when building the model. Since the data is annual, there isn't any seasonal trend and we can see the observed trend is what should be considered.

![](figures/usa_epidemics_time_series_decomp.png)


I then ran a grid search to optimize the parameters. The optimal parameters for considering the trend and seasonal trend were (0, 1, 1) and (0, 1, 1, 12). These paramters were selected based on the value of the AIC score, when compared to the AIC scores for other combinations of parameters.

I built my model with these parameters and predicted the number of deaths expected from 2002-2017. From there, I compared them to the observed counts for those years. These results are as follows:

![](figures/usa_epidemics_time_series_predict.png)

Overall, the predictions don't look half bad. The 95% confidence interval captures the observed counts in most of the predicted years. Additionally the reported Mean Square Error of the predictions is 264.36 and the Root Mean Square Error is 16.26. Given that the deaths ranged from 0 to >300, I think the RMSE is decent. However, it is worth noting at this point the downward trend of the data. There were no reported deaths from an epidemic reported in the U.S. from 2015-2017. The confidence interval does contain negative values for these years, and even some of the preceding years. This is obviously realistically impossible.

After confirming the model seemed to be making sense, at least mathematically, I then predicted the number of deaths the U.S. would experience in 2018, 2019, 2020, 2021, and 2022 along with their 95% confidence intervals. The results are as follows:

```{r forecast, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
home <- getwd()
f <- read_csv(paste(home,"/derived_data/usa_epi_5yr_forecast.csv",sep=""))
print(f)
```


![](figures/usa_epidemics_time_series_forecast.png)

The predicted number of deaths for this year according to this model is -23 with a 95% CI (-74.4, 28.6), so there you have it, 23 people will have come back to life by the end of the year.

Going into this analysis, I knew the prediction was going to be very, very far off from where we are. How could it not given the data and the trends from the dataset I was using? Again, this was my first attempt ever at a time series analysis so it was difficult for me to make choices regarding what model to use, how to hypertune parameters, and how to evaluate the quality. I was heavily reliant on online sources to guide me in the right direction but I don't have much of an intution on these decisions at this point in time. Nonetheless, I'm content with the result I've produce for this first pass and this was a good introduction to this subject.